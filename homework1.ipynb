{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qbbsDMKyNC"
      },
      "source": [
        "\n",
        "# Homework 1: Intro to Regular Expressions and Tokenization\n",
        "## Total Points: 66 points\n",
        "- **Overview**: This week we will start writing some code! We will be using Python Notebooks for most of this course, and this assignment examines your skills writing regular expressions and string processing in Python.\n",
        "\n",
        "- **Relevant Textbook**: This homework assignment corresponds to the reading in _Speech and Language Processing_ Chapter 2: [Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf).\n",
        "\n",
        "- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` and run the cell with `#PennGrader` note. **There will be no hidden tests in this assignment.** In other words, you will know your score once you finish all the `#TODO` and run all the `#PennGrader` tests!\n",
        "## To get started, **make a copy** of this colab notebook into your google drive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDv9qN5c9357"
      },
      "source": [
        "## Setup 1: PennGrader Setup - 4 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3VjTnZ05M184"
      },
      "outputs": [],
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install huggingface_hub datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jpSjR2N19kk4"
      },
      "outputs": [],
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install penngrader-client==0.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FvPA8Z2D9ki_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6ace93-f245-4f67-a3dd-bd6ebedebbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing notebook-config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xbeXT4Oj9kg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99da5364-4afe-4a32-b217-1fc9cfdfd1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
            "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
          ]
        }
      ],
      "source": [
        "!cat notebook-config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OceP0Hr-9kfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4411010c-e090-4bdb-e591-ec676e713a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 73251950\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "## TODO: enter your Penn-ID\n",
        "STUDENT_ID = 73251950 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "\n",
        "SECRET = STUDENT_ID\n",
        "grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW1', STUDENT_ID, SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k5lAgKaYa-uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c1958c-1902-4532-d331-02bfd0ac34b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# check if the PennGrader is set up correctly\n",
        "# do not chance this cell, see if you get 4/4!\n",
        "name_str = 'Z J' # TODO: enter your name\n",
        "grader.grade(test_case_id = 'name_test', answer = name_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHeB9GLB-PO1"
      },
      "source": [
        "## Setup 2: Packages\n",
        "- **Run the following cells without changing anything!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E6qSD12uEx4T"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "import requests\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from dill.source import getsource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W6zIq0k1DYZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835d8ef2-7513-4425-cd5e-712a7499a75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAaWCaMtxgtf"
      },
      "source": [
        "# Section 1. Regular Expressions Basics [15 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9BTiWiXHZDk"
      },
      "source": [
        "\n",
        "- [Regular Expression Cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)\n",
        "- [Python Official RE Guide](https://docs.python.org/3/library/re.html)\n",
        "- [W3School Re Guide](https://www.w3schools.com/python/python_regex.asp)\n",
        "- [Regex Exercises](https://www.w3resource.com/python-exercises/re/)\n",
        "\n",
        "![Benjamin Bannekat](https://www.pixelsham.com/wp-content/uploads/2020/05/REGEX.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZl7PgJ4If6b"
      },
      "source": [
        "In this section, we will explore basic usage of regular expressions in natural language processing, including three main functionalities:\n",
        "- `search`: find the first matching pattern (if there is a match anywhere in the string)\n",
        "- `findall`: find all of the matching patterns\n",
        "- `sub`: replaces one or many matches with a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JbG8ozvSF--s"
      },
      "outputs": [],
      "source": [
        "### Helper function ###\n",
        "### DO NOT CHANGE ###\n",
        "from bs4 import BeautifulSoup\n",
        "def get_text_from_url(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()\n",
        "    text = soup.get_text()\n",
        "\n",
        "    text = re.sub('\\n+', '\\n', text)\n",
        "    return text\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgz1ebO_QwNW"
      },
      "source": [
        "## 1.1 re.search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_qYtVIUUI3-"
      },
      "source": [
        "Your very first task is utilizing `re.search()` to find specific patterns from a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7otylXfoR5aK"
      },
      "source": [
        "- **Problem 1.1:** search for the word \"food\"\n",
        "    - **Overview**: Implement the function `find_food()` that determines if the word \"food\" appears in the string.\n",
        "    - **Input**: a string\n",
        "    - **Output**: `True` or `False`\n",
        "    - **Requirements**:\n",
        "        - Functionality: The function returns True if the word \"food\" is found in the input string, and False otherwise. It also accepts **potential typos**, e.g. \"fooood\".\n",
        "        - To be specific, valid \"food\" entry should have 1) one \"f\" at the beginning 2) two or more \"o\"s 3) one \"d\" at the end\n",
        "    - **Examples**:\n",
        "        - `find_food(\"foooood\")`: `True`\n",
        "        - `find_food(\"foodddd\")`: `False`\n",
        "        - `find_food(\"fod\")`: `False`\n",
        "        - `find_food(\"food!!!\")`: `True`\n",
        "        - `find_food(\"I like fooood!!\")`: `True`\n",
        "        - `find_food(\"foodie\")`: `False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oRY4SptyQtwM"
      },
      "outputs": [],
      "source": [
        "def find_food(text):\n",
        "    \"\"\"\n",
        "    Search for the word \"food\" in the text. More than two \"o\"s is also accepted.\n",
        "    Use re.search()\n",
        "\n",
        "    @param text: the string to be searched\n",
        "    @return: True if \"food\" is found, False if not\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pattern = r'foo+d(?![a-zA-Z])'\n",
        "    return re.search(pattern, text) is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5hExkHl2SstJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d9ded0-e715-4c6b-c03e-db30a5b145de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q1_food_regex_function', answer = getsource(find_food))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQBTs_7UB5b"
      },
      "source": [
        "## 1.2 re.findall\n",
        "The second exercise is using `re.findall()` to find all non-overlapping matches of pattern in string.\n",
        "\n",
        "As an example, we will first take a look at this [website](https://www.presidentsusa.net/birth.html), which stores all the birthdays of U.S presidents. We will try to parse all the birthdays from this website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LvYX80e0PQaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a16590-bc33-45fb-9c8a-5c2e767f4da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√Ø¬ª¬ø\n",
            "U.S. Presidents Birthplaces, Birthdate, and Death information\n",
            " \n",
            " \n",
            "U.S. Presidents Birthplaces and Location of Death\n",
            "The list below includes the place where every president was born and died and the dates for the birth and death. Clicking the birthplace location will display photographs of the site where each president was born.\n",
            "President\n",
            "Birth Date\n",
            "Birth Place\n",
            "Death Date\n",
            "Location of Death\n",
            "George Washington\n",
            "Feb 22, 1732\n",
            "Westmoreland Co., Va.\n",
            "Dec 14, 1799\n",
            "Mount Vernon, Va.\n",
            "John Adams\n",
            "Oct 30, 1\n"
          ]
        }
      ],
      "source": [
        "### Read text data ###\n",
        "### DO NOT CHANGE ###\n",
        "web_text = get_text_from_url('https://www.presidentsusa.net/birth.html')\n",
        "print(web_text[:500]) # let's print the first 500 char\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VdZhowYJPVRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c27f605-1ecb-4cd7-f3a8-76d630c9a380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-1146237472.py:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  date_pattern = '[A-Za-z]{3,4} \\d{1,2}, \\d{4}' # can you understand what it is trying to do?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# find all the birthdays in the website\n",
        "date_pattern = '[A-Za-z]{3,4} \\d{1,2}, \\d{4}' # can you understand what it is trying to do?\n",
        "all_dates = re.findall(date_pattern, web_text)\n",
        "len(all_dates) # how many birthdays are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "un-1gFYDTq84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b5f3f1-7bd3-42a0-c18a-8bedab3ca6bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Feb 22, 1732',\n",
              " 'Dec 14, 1799',\n",
              " 'Oct 30, 1735',\n",
              " 'July 4, 1826',\n",
              " 'Apr 13, 1743']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "all_dates[:5] # check out some of the matches!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNxCvxnkLo_E"
      },
      "source": [
        "Now it's your turn. Please use `re.findall()` and try to to parse email addresses.\n",
        "\n",
        "- **Problem 1.2:** search for emails\n",
        "    - **Overview**: Implement the function `find_email()` to find **all the emails** from the input text. We will look for emails of the faculties at [Penn Engineering Online](https://online.seas.upenn.edu/about/faculty-directory/)\n",
        "    - **Input**: a string\n",
        "    - **Output**: a list of strings\n",
        "    - **Requirements**:\n",
        "        - Return a list of valid emails found in the input string\n",
        "        - You may assume all the emails end with `.edu` or `.com`\n",
        "        - You may use no more than 2 regex patterns\n",
        "    - **Caveat**: When using `findall()`, be careful of using parentheses. Anything you have in parentheses () will be a capture group, which may have different behavior than you expect. Check [the documentation](https://docs.python.org/3/howto/regex.html) for detailed explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2iL5-r0CJXIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d7a648-bcae-4d2f-ab52-bf8b7f7bb47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aculty Directory\n",
            "Home\n",
            "            About Us          \n",
            "Faculty Directory\n",
            "              About Us            \n",
            "Faculty Leadership \n",
            "Boon Thau Loo\n",
            "RCA Professor, Department of Computer and Information Science\n",
            "Senior Associate Dean for Education and Global Initiatives ‚Äî School of Engineering and Applied Science\n",
            "Director, Distributed Systems Laboratory\n",
            "Email: boonloo@cis.upenn.edu \n",
            "Tom Farmer\n",
            "Program Director, Online Master of Computer and Information Technology\n",
            "Senior Lecturer, Department of Electrical and Systems Engineering & Department of Computer and Information Science\n",
            "Email: tfarmer@seas.upenn.edu \n",
            "James C. Gee\n",
            "Program Director, Online Master of Science in Engineering in Data Science\n",
            "Professor, Department of Radiology (Perelman School of Medicine) and Department of Computer and Information Science (School of Engineering and Applied Science)\n",
            "Director, Penn Image Computing and Science Laboratory | Co-Director, Translational Biomedical Imaging Center | Director, Interfac\n"
          ]
        }
      ],
      "source": [
        "### Read text data ###\n",
        "### DO NOT CHANGE ###\n",
        "url = 'https://online.seas.upenn.edu/about/faculty-directory/'\n",
        "penn_online_text = get_text_from_url(url)\n",
        "print(penn_online_text[1620:2600]) # check what it looks like\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "akPKglsELvMn"
      },
      "outputs": [],
      "source": [
        "def find_email(text):\n",
        "    \"\"\"\n",
        "    Find all the valid email addresses from the text, which ends with .edu or .com\n",
        "    Use re.findall() with no more than 2 patterns\n",
        "\n",
        "    @param text: the string to be searched\n",
        "    @return: a list of valid emails\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.(?:edu|com)\\b'\n",
        "    email_lst = re.findall(pattern, text)\n",
        "    return email_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VOPyUc_NLuI3"
      },
      "outputs": [],
      "source": [
        "emails_found = find_email(penn_online_text)\n",
        "assert 'ccb@upenn.edu' in emails_found, \"CCB's email not found\" # this should return nothing if CCB's email is found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "spXAio-rL1D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d2d2dd-6d65-4e44-e17f-e3d3bcba403a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q12_email_regex_function', answer = emails_found)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6wNPjpBJABH"
      },
      "source": [
        "## 1.3 re.sub\n",
        "\n",
        "\n",
        "In this problem, we will work with the Wikipedia webpage containing a [List of people from New Jersey](https://simple.wikipedia.org/wiki/List%20of%20people%20from%20New%20Jersey).\n",
        "\n",
        "In this website, people who are still living are originally labeled with their birth year. Here, your task is to use the `re.sub()` function to label them as \"still active nowadays\". For example, \"Gerard Way (born 1977)\" should be updated to \"Gerard Way (still active nowadays)\".\n",
        "\n",
        "Please note that we don't want to change the format for people who are no longer living, such as \"Sarah Vaughan (1924‚Äì1990)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UrX3j0CNK93O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0774990d-59d3-4202-86ca-54d040e3ed26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "us (born 1988), safety for the Detroit Lions (Union)\n",
            "Joseph Alexander Adams (1803‚Äì1880), engraver (born in New Germantown)[1]\n",
            "Mike Adams (born 1981), safety for the Indianapolis Colts (Paterson)\n",
            "Timothy Adams (born 1967), actor, Sunset Beach, Ocean Ave. (Belleville)\n",
            "Charles Addams (1912-1988), cartoonist; creator of The Addams Family (Westfield)\n",
            "Jordan Alan (born 1967), filmmaker (Bayonne)\n",
            "Mitch Albom (born 1958), writer, broadcaster, and musician (Passaic)\n",
            "Edwin \"Buzz\" Aldrin (born 1930) NASA, astronaut, second man to walk on the moon (born in Glen Ridge, grew up in Montclair)\n",
            "Jason Alexander (born 1959), actor, George Costanza on Seinfeld (Newark, raised in Livingston)\n",
            "Samuel Alito (born 1950), U.S. Supreme Court justice (Trenton, raised in Hamilton)\n",
            "Malik Allen (born 1978), NBA player (Willingboro)\n",
            "John Amos (bo\n"
          ]
        }
      ],
      "source": [
        "### Read text data ###\n",
        "### DO NOT CHANGE  ###\n",
        "q13_text = get_text_from_url('https://simple.wikipedia.org/wiki/List_of_people_from_New_Jersey')\n",
        "print(q13_text[1913:2739]) # check what it looks like\n",
        "### DO NOT CHANGE  ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtevvqGsHcJm"
      },
      "source": [
        "- **Problem 1.3**: substitute date format for all active people\n",
        "    - **Overview**: Implement the function `change_date_format()` to replace all \"(born xxxx)\" with \"(still active nowadays)\" in the input text.\n",
        "    - **Input**: text that contains information of notable people from [New Jersey](https://simple.wikipedia.org/wiki/List%20of%20people%20from%20New%20Jersey) (`q13_text`)\n",
        "    - **Output**: the modified document as a string\n",
        "    - **Example**:  \n",
        "\n",
        "```\n",
        "\"...\n",
        "Isa Abdul-Quddus (born 1988), safety for the Detroit Lions (Union)\n",
        "Joseph Alexander Adams (1803‚Äì1880), engraver (born in New Germantown)[1]\n",
        "Mike Adams (born 1981), safety for the Indianapolis Colts (Paterson)\n",
        "...\"\n",
        "```\n",
        "\n",
        "\n",
        "->\n",
        "\n",
        "\n",
        "```\n",
        "\"...\n",
        "Isa Abdul-Quddus (still active nowadays), safety for the Detroit Lions (Union)\n",
        "Joseph Alexander Adams (1803‚Äì1880), engraver (born in New Germantown)[1]\n",
        "Mike Adams (still active nowadays), safety for the Indianapolis Colts (Paterson)\n",
        "...\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5Bi8j9h5GdXR"
      },
      "outputs": [],
      "source": [
        "def change_date_format(text):\n",
        "    \"\"\"\n",
        "    Replace all the \"(born xxxx)\" with \"(still active nowadays)\" in the text.\n",
        "    Use re.sub()\n",
        "\n",
        "    @param text: the string to be searched\n",
        "    @return: the modified document\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pattern = r'\\(born\\s+\\d{4}\\)'\n",
        "    modified_text = re.sub(pattern, '(still active nowadays)', text)\n",
        "    return modified_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qSxyuXvpPWwi"
      },
      "outputs": [],
      "source": [
        "q13_text_sub = change_date_format(q13_text)\n",
        "assert \"born 19\" not in q13_text_sub[1913:2785], \"not correctly modified\"\n",
        "assert \"still active nowadays\" in q13_text_sub[1913:2785], \"not correctly modified\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Dh3T2Rc-PNGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685d9fda-695f-4c25-ff54-2c3171c6992d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q13_sub_regex_function', answer = q13_text_sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ajo37bOBKrj"
      },
      "source": [
        "# Section 2. Tokenization [47 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmvUD6P3BYXF"
      },
      "source": [
        "## 2.1 Naive Way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KBLIpmDBZZO"
      },
      "source": [
        "### 2.1.1 Whitespace Tokenizer - 2 points\n",
        "Word tokenization is the process of splitting a large sample of text into words. This is a requirement in natural language processing tasks where each word needs to be captured and subjected to further analysis like classifying and counting them for a particular sentiment etc.\n",
        "\n",
        "Whitespace Tokenizer is the most basic tokenizer which splits strings on ICU-defined whitespace characters (eg. space, tab, new line).\n",
        "This is often good for quickly building out prototype models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFfjHSxOSdz"
      },
      "source": [
        "- **Problem 2.1.1** Whitespace Tokenizer\n",
        "  - **Overview**: Implement the function `whitespaceTokenizer()` to tokenize the sentence by splitting with whitespace characters\n",
        "  - **Input**: a string\n",
        "  - **Output**: a list of strings\n",
        "  - **Requirements**:\n",
        "      - Split the input text by whitespaces and return a list of tokens\n",
        "      - The function should also be able to recognize other forms of whitespaces, such as `\\n`\n",
        "      - Please only use Python's built-in functions. NLTK and other packages should not be used here\n",
        "  - **Example**: `\"This is a test\"` -> `[\"This\", \"is\", \"a\", \"test\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VHKwG11sBKSA"
      },
      "outputs": [],
      "source": [
        "def whitespaceTokenizer(text):\n",
        "    \"\"\"\n",
        "    Tokenize the sentence by splitting with whitespace characters\n",
        "    Use built-in functions only\n",
        "\n",
        "    @param text: the string to be tokenized\n",
        "    @return: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    tokens = text.split()\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_G3MDZd9uSgj"
      },
      "outputs": [],
      "source": [
        "text = \"This is a test\"\n",
        "assert whitespaceTokenizer(text) == [\"This\", \"is\", \"a\", \"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vNSlfEdMBizj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e09ce1-b60f-4707-d8c6-8d87ef370d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q211_whitespace_tokenizer_function', answer = getsource(whitespaceTokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpFxmzWsC5ns"
      },
      "source": [
        "### 2.1.2 Update Whitespace Tokenizer - 2 points\n",
        "\n",
        "We see that seperating words with spaces works sometimes, but what if there are punctuations? With the most basic implementation in 2.1.1, `\"This, is a test!\"` will become `[\"This,\", \"is\", \"a\", \"test!\"]` after tokenization. This is not what we usually want. Punctuation is often treated as a special token. For instance, periods and exclamation points mark the end of a sentence. Therefore, naturally we want them to be separated with words during tokenization.\n",
        "\n",
        "Please implement `whitespaceTokenizerUpdate()` as an updated version that separates all the punctuations when splitting words with whitespaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho2sPJ-JbIcD"
      },
      "source": [
        "- **Problem 2.1.2** Update Whitespace Tokenizer\n",
        "  - **Overview**: Implement the function `whitespaceTokenizerUpdate()` to tokenize the sentence by splitting the whitespace characters and separating punctuation at the same time\n",
        "  - **Input**: a string\n",
        "  - **Output**: a list of strings\n",
        "  - **Requirements**:\n",
        "      - Split the input text by whitespaces and return a list of tokens\n",
        "      - The function should also be able to recognize other forms of whitespaces, such as `\\n`, and separate punctuations from words\n",
        "      - Please only use Python's built-in functions. NLTK and other packages should not be used here\n",
        "      - You may assume there are only common English punctuations\n",
        "  - **Example**: `\"This 212, is a test!\"` -> `[\"This\", \"212\", \",\" \"is\", \"a\", \"test\", \"!\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W9iUxuWeBmG6"
      },
      "outputs": [],
      "source": [
        "def whitespaceTokenizerUpdate(text):\n",
        "    \"\"\"\n",
        "    Tokenize the sentence by splitting with whitespace characters\n",
        "    Punctuations should be separated\n",
        "\n",
        "    @param text: the string to be tokenized\n",
        "    @return: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "\n",
        "    for char in punctuations:\n",
        "        text = text.replace(char, \" \" + char + \" \")\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rygJJz2eucAB"
      },
      "outputs": [],
      "source": [
        "text = \"This 212, is a test!\"\n",
        "assert whitespaceTokenizerUpdate(text) == [\"This\", \"212\", \",\", \"is\", \"a\", \"test\", \"!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nnIOsabVC91W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5326a467-2dc3-48c7-ae78-e0347ffc5977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q212_whitespace_tokenizer_update', answer = getsource(whitespaceTokenizerUpdate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkfTD4MtDEO2"
      },
      "source": [
        "### 2.1.3 Tokenize with NLTK Library - 4 points\n",
        "You must have noticed that tokenization can be very tricky depending on your needs. With the updated whitespace tokenizer, you could probably get desirable results in 95% of the tasks with someting super simple, and 99% with an extra 20-60 minutes of updating. It may require a lot more to get to 99.9%.\n",
        "\n",
        "Luckily, there are many useful resources from the NLTK library that can help with this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIg83ChmcelF"
      },
      "source": [
        "- **Problem 2.1.3** NLTK's word tokenizer.\n",
        "  - **Overview**: Implement the function `nltk_tokenize_content()` to tokenize sentences\n",
        "  - **Input**: a string\n",
        "  - **Output**: a list of strings\n",
        "  - **Requirements**:\n",
        "      - Convert the input text to lower case\n",
        "      - Utilize NLTK's `work_tokenize()` function\n",
        "      - Remove all the non-words and stop words (hint: non-words: words are not purely alphabetic. In other words, a 'word' should be purely alphabetic.)\n",
        "  - **Example**: `\"Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).\"` -> `['good', 'muffins', 'cost', 'new', 'york', 'please', 'buy', 'two', 'thanks']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PRBCs4hpDGhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57f621b-8932-4479-c203-36b463d35717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "### Download Resources ###\n",
        "### DO NOT CHANGE ###\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5je1B0QKDHlz"
      },
      "outputs": [],
      "source": [
        "def nltk_tokenize_content(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text with nltk's word tokenizer\n",
        "    Convert the text to lowercase and remove all non-words and stopwords\n",
        "    see https://www.nltk.org/howto/tokenize.html\n",
        "\n",
        "    @param text: a string for tokenization\n",
        "    @return: a list of tokens\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # 1) lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2) nltk tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3) keep purely alphabetic words & remove stopwords\n",
        "    tokens = [tok for tok in tokens if tok.isalpha() and tok not in stopwords]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m392mCrtx7bL"
      },
      "outputs": [],
      "source": [
        "text = \"Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).\"\n",
        "assert nltk_tokenize_content(text) == ['good', 'muffins', 'cost', 'new', 'york', 'please', 'buy', 'two', 'thanks']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EKDdw_gqDYOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663426fc-a427-48cf-f530-c8fdeec3e581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "### Test cases ###\n",
        "### DO NOT CHANGE ###\n",
        "test_sents = [\n",
        "    \"The 19-year-old singer was caught on camera being escorted out of the store by security guards.\",\n",
        "    \"‚ÄúA program is never less than 90% complete, and never more than 95% complete.‚Äù‚Äî Terry Baker's quote'\",\n",
        "    \"To train such a large model, OpenAI crawled 40GB worth of text from the web (roughly 20,000,000,000 words).\",\n",
        "    \"text = 'God is Great! I won a lottery.'; 'id': 22189,\"\n",
        "]\n",
        "test_results = [nltk_tokenize_content(sent) for sent in test_sents]\n",
        "### DO NOT CHANGE ###\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q213_nltk_tokenizer_function', answer = test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrKJs4EDdHx"
      },
      "source": [
        "As a side note, one commonly used tokenization standard is known as the Penn Treebank tokenization standard, which uses regular expressions to tokenize text.\n",
        "\n",
        "This standard has the following features:\n",
        "\n",
        "- split standard contractions, e.g. doesn't -> does & n't; they'll -> they & 'll\n",
        "- keep hyphenated words together\n",
        "- treat most punctuation characters as separate tokens\n",
        "- split off commas and single quotes, when followed by whitespace\n",
        "- separate periods that appear at the end of a line\n",
        "\n",
        "In practice, since tokenization needs to be run before any other language processing, it needs to be very fast. The standard method for tokenization is therefore to use deterministic algorithms based on regular expressions compiled into very efficient finite state automata.\n",
        "\n",
        "With regular expressions, it is very convenient to customize the tokenization algorithm with the NLTK library. For example, instead of removing all stop words and non-word tokens, we may want to keep a selective set of tokens. e.g. percentages and complex words:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gvTWXcSyDdry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046f39d0-7d33-4cf0-f207-e2ae96329268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
          ]
        }
      ],
      "source": [
        "### EXAMPLE ###\n",
        "### DO NOT CHANGE ###\n",
        "text = 'That U.S.A. poster-print costs $12.40...'\n",
        "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
        "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
        "      | \\.\\.\\.              # ellipsis\n",
        "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "    '''\n",
        "print(nltk.regexp_tokenize(text, pattern))\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZathogDjFM"
      },
      "source": [
        "## 2.2 Tokenize with Stanza/allennlp Library - 5 points\n",
        "[Stanza](https://stanfordnlp.github.io/stanza/tokenize.html) is another useful tool for word tokenization. Upon feeding it raw text, Stanza will:\n",
        "- Tokenize it and groups tokens into sentences as the first step of processing\n",
        "- Unlike other existing toolkits, Stanza combines tokenization and sentence segmentation from raw text into a single module.\n",
        "(This is done to predict the position of words in a sentence, as use of words are context-sensitive in some languages.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PvTBmZvzDkHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "acbb1f15-9e87-411e-b0d0-b56ce4c746c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "### Import stanza ###\n",
        "### DO NOT CHANGE ###\n",
        "%%capture\n",
        "!pip install stanza==1.5.0\n",
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7JhlDxDoWk"
      },
      "source": [
        "- **Problem 2.2**: Stanza tokenizer\n",
        "\n",
        "  - **Overview**: Implement the function `stanza_tokenizer()` to tokenize a sentence using the stanza tokenizer.\n",
        "  - **Input**: a string\n",
        "  - **Output**: a list of strings\n",
        "  - **Requirements**:\n",
        "      - Utilize Stanza's pipeline (the variable `nlp` in the above cell)\n",
        "      - Check the documentation [here](https://stanfordnlp.github.io/stanza/tokenize.html)\n",
        "  - **Example**: `\"Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).\"` -> `['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')', 'me', 'two', 'of', 'them', '.', '(', 'Thanks', ')', '.']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XSALiix1Dqd3"
      },
      "outputs": [],
      "source": [
        "def stanza_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text with stanza's word tokenizer\n",
        "    see https://stanfordnlp.github.io/stanza/tokenize.html\n",
        "\n",
        "    @param text: the string for tokenization\n",
        "    @return: a list of tokens\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            tokens.append(word.text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vTwNJFiZyli0"
      },
      "outputs": [],
      "source": [
        "text = \"Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).\"\n",
        "assert stanza_tokenizer(text) == ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')', 'me', 'two', 'of', 'them', '.', '(', 'Thanks', ')', '.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vUtUma5ADzHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced4f96f-65e5-457f-eddb-88d5198da31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "### DO NOT CHANGE ###\n",
        "test_sents = [\n",
        "    'Antidisestablishmentarianism',\n",
        "    'Conversationalization',\n",
        "    \"Most have seen this as Trump partisanizing the issue, much as he has with masks.\",\n",
        "    \"‚ÄúA program is never less than 90% complete, and never more than 95% complete.‚Äù‚Äî Terry Baker's quote'\",\n",
        "    \"This is a test sentence for stanza. This is another sentence.\"\n",
        "]\n",
        "test_results = [stanza_tokenizer(sent) for sent in test_sents]\n",
        "### DO NOT CHANGE ###\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q22_stanza_tokenizer_function', answer = test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHW8d04yECBz"
      },
      "source": [
        "## 2.3 Subword tokenizers (Byte-Pair Encoding) - 34 points\n",
        "\n",
        "Whole word tokenization algorithms aren't perfect, because some languages have many word-forms and words modification for each word, such as prefixes and suffixes. We want to save morphology information in the text, but to save every possible word-form isn't memory-efficient or easy to train. Even then, we might still run into unknown words during inference.\n",
        "\n",
        "Alternatively, subword can be used with a smaller vocabulary, and allow the model to have some information about novel words from the subwords that create them. We can create a tokenziation mechanism that will tokenize every word by subword morphology. The unsupervised algorithm that does so called is Byte Pair Encoding. Here is how it works:\n",
        "\n",
        "![](https://lena-voita.github.io/resources/lectures/seq2seq/bpe/build_merge_table.gif)\n",
        "\n",
        "1. Split text into individual characters\n",
        "2. Begin BPE with a vocabulary that is just the set of all individual characters.\n",
        "3. Choose the two symbols from the vocabulary that are most frequently adjacent in the corpus\n",
        "4. Merge the most popular pair and update the corpus with the new merge pair\n",
        "5. Continue to step 3 until we reach given vocabulary size.\n",
        "\n",
        "It's easy algorithm, and we have several implementations:\n",
        "- SentencePiece\n",
        "- fastBPE\n",
        "- Tokenizers by Huggingfaceü§ó\n",
        "- YouTokenToMe\n",
        "\n",
        "We will now implement our BPE tokenization from scratch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YMy7VCzEF4B"
      },
      "source": [
        "### 2.3.1 Wikipedia Simple Dataset Word Frequency - 5 points\n",
        "\n",
        "We'll use this [Wikipedia Simple Dataset](https://huggingface.co/datasets/dangne/processed-wikipedia-20220301.simple) as our training data for BPE tokenization. To begin with, we need to compute word frequencies which will guide merging during tokenization.\n",
        "\n",
        "- **Problem 2.3.1:** Compute word frequencies\n",
        "    - **Overview**: Implement the function `compute_word_freq()` to compute the frequencies of each word in the Wikipedia Simple dataset.\n",
        "    - **Input**: a list of strings (sentences), an int as threshold\n",
        "    - **Output**: a dictionary with words as keys and counts as values\n",
        "    - **Requirements**:\n",
        "        - Iterate over the corpus\n",
        "        - Tokenize each sentence with your `nltk_tokenize_content()` function in 2.1.3 to get a list of words\n",
        "        - Count the frequency of each word\n",
        "        - **Only keep a word if its frequency is equal or bigger than the threshold (50 by default)**\n",
        "    - **Example**:\n",
        "    \n",
        "\n",
        "```\n",
        ">>> lst = ['Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).', \\\n",
        "'Amidst the vibrant streets of New York, a cozy bakery entices passersby with the scent of freshly baked blueberry muffins.']\n",
        ">>> dic = compute_word_freq(lst, threshold = 2)\n",
        ">>> print(dic)\n",
        "{'muffins': 2, 'new': 2, 'york': 2}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3PzjOz2019T-"
      },
      "outputs": [],
      "source": [
        "### Load data ###\n",
        "### DO NOT CHANGE ###\n",
        "%%capture\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "# see https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html#from-local-files\n",
        "data = load_dataset(\"dangne/processed-wikipedia-20220301.simple\")\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "eQZw39ciEIGG"
      },
      "outputs": [],
      "source": [
        "def compute_word_freq(corpus, threshold = 50):\n",
        "    \"\"\"\n",
        "    Compute the frequencies of each word in the corpus after tokenization\n",
        "    Keep only words with frequencies >= threshold\n",
        "\n",
        "    @param1 corpus: a list of strings\n",
        "    @param2 threshold: an int indicating minimum frequency accepted\n",
        "    @return: a dictionary with words as keys and frequencies as values\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "def compute_word_freq(corpus, threshold=50):\n",
        "    word_freqs = {}\n",
        "\n",
        "    for sentence in corpus:\n",
        "        words = nltk_tokenize_content(sentence)\n",
        "\n",
        "        for w in words:\n",
        "            word_freqs[w] = word_freqs.get(w, 0) + 1\n",
        "\n",
        "\n",
        "    word_freqs = {w: c for w, c in word_freqs.items() if c >= threshold}\n",
        "\n",
        "    return word_freqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4DZSjOrz7dVS"
      },
      "outputs": [],
      "source": [
        "lst = ['Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).', \\\n",
        "'Amidst the vibrant streets of New York, a cozy bakery entices passersby with the scent of freshly baked blueberry muffins.']\n",
        "dic = compute_word_freq(lst, threshold = 2)\n",
        "assert dic == {'muffins': 2, 'new': 2, 'york': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eOGHowCc0lcp"
      },
      "outputs": [],
      "source": [
        "### Compute word_freqs ###\n",
        "### DO NOT CHANGE ###\n",
        "corpus = [dp['text'] for dp in data['train']]\n",
        "word_freqs = compute_word_freq(corpus) # this line might take up to 10 minutes to finish, might be worth it to test it on a small corpus first (e.g. a few sentences)\n",
        "### DO NOT CHANGE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SORgaOcqAwC4"
      },
      "outputs": [],
      "source": [
        "assert len(word_freqs) == 24900"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "syNPp9OwEMpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58752b59-ded8-485f-ab93-833c6d439a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "### DO NOT CHANGE ###\n",
        "test_corpus = [\n",
        "    \"The cat and dog playfully chased each other around the yard, showing their playful nature.\",\n",
        "    \"While the cat prefers to nap, the dog's favorite pastime is to play fetch in the park.\",\n",
        "    \"At the pet-friendly cafe, you can watch as the cat and dog interact and play with their toys.\",\n",
        "    \"The children laughed joyfully as they watched their pet cat and dog play together in the living room.\",\n",
        "    \"In the early morning, you can often see the cat and dog playfully exploring the garden side by side.\"\n",
        "]\n",
        "test_word_freqs = compute_word_freq(test_corpus, 3)\n",
        "### DO NOT CHANGE ###\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q231_word_freq_function', answer = (test_word_freqs, word_freqs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QqTaQPEaqL"
      },
      "source": [
        "### 2.3.2 Construct an Alphabet for the Corpus - 4 points\n",
        "\n",
        "Now we have our corpus with all the frequencies. The next step is to build the vocabulary. As a very simple example, let's say our corpus uses these two words: `['april', 'proud']`, the base vocabulary would be `['a', 'd', 'i', 'l', 'o', 'p', 'r', 'u']`. For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well.\n",
        "\n",
        "Why do we need the vocabulary? Recall that the most frequent pairs will be added to the base vocabulary during training. The BPE algorithm will keep running until we reach a certain vocabulary size.\n",
        "\n",
        "For the convenience of training, here we will also split each word separately into individual characters. This will be used to compute the most frequent pairs.\n",
        "\n",
        "- **Problem 2.3.2-1**: Build vocabulary\n",
        "    - **Overview**: Implement The function `compute_vocab()` to build the base vocabulary for the corpus\n",
        "    - **Input**: a dictionary with words as keys and counts as values (`word_freqs`)\n",
        "    - **Output**: a list of characters\n",
        "    - **Requirements**:\n",
        "        - Sort the base vocabulary in lexicographic order.\n",
        "        - Please don't modify `word_freqs` in-place.\n",
        "    - **Example**: `{'april': 2, 'proud': 3}` -> `['a', 'd', 'i', 'l', 'o', 'p', 'r', 'u']`\n",
        "- **Problem 2.3.2-2**: Split corpus\n",
        "    - **Overview**: - Implement the function `corpus_splitter()` to split each word into individual characters in `word_freqs`\n",
        "    - **Input**: a dictionary with words as keys and counts as values (`word_freqs`)\n",
        "    - **Output**: a dictionary with words as keys and **a list of characters** as values\n",
        "    - **Requirements**:\n",
        "        - Please don't modify `word_freqs` in-place. Return a new dictionary\n",
        "    - **Example**: `{'april': 2, 'proud': 3}` -> `{'april': ['a','p', 'r', 'i', 'l'], 'proud': ['p', 'r', 'o', 'u', 'd']}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "54PBUgwewpYE"
      },
      "outputs": [],
      "source": [
        "def compute_vocab(word_freqs):\n",
        "    \"\"\"\n",
        "    Compute the base vocabulary from all words\n",
        "\n",
        "    @param word_freqs: the dictionary we get in 2.3.1\n",
        "    @return: a list of all unique characters\n",
        "    \"\"\"\n",
        "    vocab_set = set()\n",
        "\n",
        "    for word in word_freqs.keys():\n",
        "        for ch in word:\n",
        "            vocab_set.add(ch)\n",
        "\n",
        "    base_vocab = sorted(vocab_set)\n",
        "    return base_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4TKzwVyvw6uV"
      },
      "outputs": [],
      "source": [
        "base_vocab = compute_vocab(word_freqs)\n",
        "assert len(base_vocab) == 74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "o22Fd9zPEcmt"
      },
      "outputs": [],
      "source": [
        "def corpus_splitter(word_freqs):\n",
        "    \"\"\"\n",
        "    Split each word into individual characters (i.e. list of characters)\n",
        "    e.g. {april: [a, p, r, i, l]}\n",
        "\n",
        "    @param word_freqs: the dictionary we get in 2.3.1\n",
        "    @return: a dictionary in which keys are the words in word_freqs,\n",
        "      values are lists of characters after splitting\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    splits = {}\n",
        "\n",
        "    for word in word_freqs.keys():\n",
        "        splits[word] = list(word)\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "FlJzaQ1ZEdmx"
      },
      "outputs": [],
      "source": [
        "splits = corpus_splitter(word_freqs)\n",
        "assert splits['april'] == ['a', 'p', 'r', 'i', 'l']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aut67NjIEhIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39b81bf-edc0-495d-e173-1307ca6aa3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q232_alphabet_size_and_spliter', answer = (base_vocab, splits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBToKlHMFK5l"
      },
      "source": [
        "### 2.3.3 Computer Token Pair Frequency - 4 points\n",
        "\n",
        "Now that we have the splitted corpus, we can compute the frequencies for each pair. Let's assume the words have the following frequencies: `{'april': 2, 'proud': 3}`. Then we look at frequencies of all pairs such as `(\"a\", \"p\")` and `(\"p\", \"r\")`. The pair `(\"a\", \"p\")` only appears in the word `'april'` so the frequency is also 2. The pair `(\"p\", \"r\")` is present in both words, so 5 times total in the corpus.\n",
        "\n",
        "It's very important to know that the the values in `splits` can include tokens with more than 1 character. For instance, after several rounds of training, the tokens of the word `'april'` could be `'april': ['ap', 'r', 'i', 'l']`. We will dig into the merge process in 2.3.4.\n",
        "\n",
        "- **Problem 2.3.3**: compute pair frequency\n",
        "    - **Overview**: Implement the function `compute_pair_freqs()` to compute the frequencies of **pairs of characters** as defined in the current vocabulary in the corpus.\n",
        "    - **Input**: `word_freqs` from 2.3.1, `splits` from 2.3.2\n",
        "    - **Output**: a dictionary with pairs (tuples) as keys and frequencies as values\n",
        "    - **Requirements**:\n",
        "        - Retrieve the pairs from `splits` and find their frequencies from `word_freqs`\n",
        "        - Please don't modify `word_freqs` and `splits`in-place\n",
        "    - **Example1**: `word_freqs = {'april': 2, 'proud': 3}; splits = {'april': ['a','p', 'r', 'i', 'l'], 'proud': ['p', 'r', 'o', 'u', 'd']}` -> `pair_freqs = {('a', 'p'): 2, ('p', 'r'): 5, ('r', 'i'): 2, ('i', 'l'): 2, ('r', 'o'): 3, ('o', 'u'): 3, ('u', 'd'): 3}`\n",
        "    - **Example2**: `word_freqs = {'april': 2, 'apricot': 3}; splits = {'april': ['ap', 'r', 'i', 'l'], 'apricot': ['ap', 'r', 'i', 'c', 'o', 't']}` -> `pair_freqs = {('ap', 'r'): 5,('r', 'i'): 5, ('i', 'l'): 2, ('i', 'c'): 3, ('c', 'o'): 3, ('o', 't'): 3}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "m_1co8K5f4eH"
      },
      "outputs": [],
      "source": [
        "def compute_pair_freqs(word_freqs, splits):\n",
        "    \"\"\"\n",
        "    Compute the frequency of pairs of tokens\n",
        "\n",
        "    @param1 word_freqs: the dictionary we get in 2.3.1\n",
        "    @param2 splits: the dictionary we get in 2.3.2\n",
        "    @return: a dictionary in which keys are pairs of tokens,\n",
        "      values are frequencies\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pair_freqs = {}\n",
        "\n",
        "    for word, freq in word_freqs.items():\n",
        "        tokens = splits.get(word, [])\n",
        "        # if a word has 0 or 1 token, it contributes no pairs\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i + 1])\n",
        "            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
        "\n",
        "    return pair_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-xsTl6mOgva2"
      },
      "outputs": [],
      "source": [
        "pair_freqs = compute_pair_freqs(word_freqs, splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Gn_aoL0C6Ww9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942020d9-b625-44e2-da06-cd5ea4a3da7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q233_pair_freqs', answer = pair_freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONS8Xmabd8-8"
      },
      "source": [
        "### 2.3.4 Merge Token Pairs in Corpus - 6 points\n",
        "\n",
        "After getting `pair_freqs`, we're able to find the most frequent pairs from it. As is mentioned previously, we will update `splits` by merge these pairs. Let's use the same example again. Suppose the `pair_freqs` is `{('a', 'p'): 2, ('p', 'r'): 5, ('r', 'i'): 2, ('i', 'l'): 2, ('r', 'o'): 3, ('o', 'u'): 3, ('u', 'd'): 3}`, we will find the most frequent pair is `('p', 'r')`. Then, the updated `splits` will be: `{'april': ['a','pr', 'i', 'l'], 'proud': ['pr', 'o', 'u', 'd']}`\n",
        "\n",
        "- **Problem 2.3.4**: Merge pair\n",
        "    - **Overview**: Implement the function `merge_pair` to merge a specified pair of tokens, and update the entire splits with the merge.\n",
        "    - **Input**: the first and second tokens to be merged; a dictionary in the same format as `splits`\n",
        "    - **Output**: the updated splits\n",
        "    - **Hint**:\n",
        "        - You can assume `(a, b)` is the most frequent pair. There's no need to find the max frequencies.\n",
        "    - **Example**: `a = 'p', b = 'r', splits = {'april': ['a','p', 'r', 'i', 'l'], 'proud': ['p', 'r', 'o', 'u', 'd']}` -> `updated_splits = {'april': ['a','pr', 'i', 'l'], 'proud': ['pr', 'o', 'u', 'd']}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DKW0-79ng055"
      },
      "outputs": [],
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    \"\"\"\n",
        "    Merge the a and b into ab and return the updated splits dict\n",
        "    This function will be used repeatedly\n",
        "\n",
        "    @param1 a: the first token to be merged\n",
        "    @param2 b: the second token to be merged\n",
        "    @param3 splits: a dictionary in which keys are pairs of tokens,\n",
        "      values are frequencies\n",
        "    @return: the updated splits\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    merged_token = a + b\n",
        "    updated_splits = {}\n",
        "\n",
        "    for word, tokens in splits.items():\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            # if we see the pair a,b at positions i,i+1 -> merge\n",
        "            if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2  # skip both a and b\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "\n",
        "        updated_splits[word] = new_tokens\n",
        "\n",
        "    return updated_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0j7L5nU_6xs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7fd03e-4e1d-4b5e-ffa4-927cd1f37ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 6/6 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "test_splits = {\n",
        "    'april': ['a', 'p', 'r', 'i', 'l'],\n",
        "    'apartment': ['a', 'p', 'a', 'r', 't', 'm', 'e', 'n', 't']\n",
        "}\n",
        "test_merge_result = merge_pair('a', 'p', test_splits)\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q234_merge_pair_function', answer = test_merge_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fevTMY7neGcd"
      },
      "source": [
        "### 2.3.5 Putting it all together - 15 points\n",
        "\n",
        "With all the building blocks above, we're ready to train. Let's review the procedure and put everything together.\n",
        "\n",
        "- **Problem 2.3.5-1**:\n",
        "    - **Overview**: Implement the training loop for BPE, iteratively finding the most frequent pair, update the corpus by merging all adjacent tokens of that pair, and repeat.\n",
        "    - **Input**: The base vocabulary: `base_vocab`; corpus: `word_freqs`; the splitted corpus: `splits`\n",
        "    - **Output**: `merge_records`, a dictionary with pairs as keys and merged pairs as values. e.g., `{('to', 'y'): 'toy'}`\n",
        "    - **Requirements**:\n",
        "        -   For each iteration of training:\n",
        "          -  Use `compute_pair_freqs()` to get the frequencies and find the most frequent pair\n",
        "          -  Use `merge_pair()` to merge that pair in `splits`\n",
        "          -  Save the learned merge rules in `merges_records`\n",
        "          -  Update the vocabulary\n",
        "          -  Repeat until the size of the vocabulary reach a certain number\n",
        "        - **Use 5000 as the vocab size to pass the autograder, but before that, try using smaller vocab size to test your code.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MFz8csP8hB4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6770ee48-b994-440e-cdeb-8a135c977433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11min 21s, sys: 636 ms, total: 11min 21s\n",
            "Wall time: 11min 29s\n"
          ]
        }
      ],
      "source": [
        "###### Part 1: Training ######\n",
        "%%time\n",
        "vocab_size = 5000 # test with smaller vocab size first, 5000 vocab would take 25-30 mins\n",
        "vocab = base_vocab.copy()  # ‚Üê ÂÖ≥ÈîÆÔºöË¶ÅÁî® .copy() Â§çÂà∂ÔºÅ\n",
        "merges_records = {} # used to save the learned merge rules, e.g.: {('to', 'y'): 'toy'}\n",
        "splits_copy = copy.deepcopy(splits) # work on a copy so that you can trace back\n",
        "\n",
        "while len(vocab) < vocab_size: # repeat until vocab size reaches 5000\n",
        "    # TODO: compute pair frequencies\n",
        "    pair_freqs = compute_pair_freqs(word_freqs, splits_copy)\n",
        "\n",
        "    # TODO: find the most frequent pair\n",
        "    most_freq_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "    a, b = most_freq_pair\n",
        "    merged_token = a + b\n",
        "\n",
        "    # TODO: update splits_copy by merging the most frequent pair\n",
        "    splits_copy = merge_pair(a, b, splits_copy)\n",
        "\n",
        "    # TODO: save merge rule\n",
        "    merges_records[(a, b)] = merged_token\n",
        "\n",
        "    # TODO: append merged pair to vocab\n",
        "    vocab.append(merged_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtjRmrgobSHe"
      },
      "source": [
        "Now, our BEP tokenizer is ready to use. To kokenize a string, simply pre-tokenize it and apply all merge rules in `merges_records`.\n",
        "\n",
        "It's very important to know that the tokenization result of BPE tokenizer depends on the merging orders. The BPE algorithm learns the merge rules in a particular order based on the frequencies of subtokens. This ordering is used greedly during the encoding process for new text. That is why we want to iterate over `merges_records` in it's original order and apply merge rules one by one.\n",
        "\n",
        "- **Problem 2.3.5-2**:\n",
        "    - **Overview**: Implment a function `BPE_tokenize()` that takes in raw text and performs BPE tokenization to return a list of tokens.\n",
        "    - **Input**: the text to be tokenized; `merges_records`\n",
        "    - **Output**: a list of strings\n",
        "    - **Requirements**:\n",
        "        - Pre-tokenize the lowercased input text into a list of tokens, **think about which tokenizer to use**\n",
        "        - Split each token in the list to a list of single characters\n",
        "        - Iterate over `merges_records` to apply all merge rules to the splits\n",
        "        - Return updated splits as a list of tokens\n",
        "    - **Example**:\n",
        "\n",
        "    ```\n",
        "    >>> text = 'penpineapple-applepen!'\n",
        "    >>> merges_records = {('i', 'n'): 'in', ('e', 'n'): 'en', ('l', 'e'): 'le',('a', 'p'): 'ap',\\\n",
        "    ('in', 'e'): 'ine', ('p', 'en'): 'pen', ('p', 'le'): 'ple', ('ap', 'ple'): 'apple', ('p', 'ine'): 'pine}\n",
        "    >>> BPE_tokenize(text, merges_records)\n",
        "    []\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-oO5juG7nNon"
      },
      "outputs": [],
      "source": [
        "###### Part 2: Apply trained tokenizer ######\n",
        "def BPE_tokenize(text, merges_records):\n",
        "    \"\"\"\n",
        "    Use the merges_records to tokenize the input text\n",
        "\n",
        "    @param text: the input text to be tokenized\n",
        "    @return: a list of tokens\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "\n",
        "    # pre-tokenize the lowercased input text,\n",
        "    # think about which tokenizer to use,\n",
        "    # it should be the same one used to create your `word_freqs`\n",
        "    text = text.lower()\n",
        "    tokenized = whitespaceTokenizerUpdate(text)\n",
        "\n",
        "    # Split each token to a list of characters\n",
        "    splits = [list(tok) for tok in tokenized]\n",
        "\n",
        "    # Iterate over merges_records and apply all merge rules\n",
        "    for (a, b) in merges_records:\n",
        "        new_splits = []\n",
        "        for tokens in splits:\n",
        "            merged = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                # Check if current and next token match the merge pair\n",
        "                if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
        "                    merged.append(a + b)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    merged.append(tokens[i])\n",
        "                    i += 1\n",
        "            new_splits.append(merged)\n",
        "        splits = new_splits\n",
        "\n",
        "    # Flatten the list of lists into a single list\n",
        "    return sum(splits, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "bmwb3aG0y3kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714bba75-5f5d-43bc-da6f-b932d3ff871c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ant', 'id', 'is', 'establish', 'ment', 'arian', 'ism']\n",
            "['con', 'vers', 'ation', 'al', 'ization']\n",
            "['international', 'ization']\n",
            "['pron', 'om', 'inal', 'ization']\n",
            "['personal', 'ization']\n",
            "['most', 'h', 'ave', 'seen', 'th', 'is', 'as', 'trump', 'part', 'is', 'an', 'iz', 'ing', 'the', 'issue', ',', 'much', 'as', 'he', 'h', 'as', 'with', 'mas', 'ks', '.']\n",
            "['part', 'is', 'an', 'iz', 'ing']\n",
            "Correct! You earned 15/15 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "### DO NOT CHANGE ###\n",
        "tests = [\n",
        "    \"Antidisestablishmentarianism\",\n",
        "    \"Conversationalization\" ,\n",
        "    \"internationalization\" ,\n",
        "    \"pronominalization\" ,\n",
        "    \"personalization\",\n",
        "    \"Most have seen this as Trump partisanizing the issue, much as he has with masks.\",\n",
        "    \"partisanizing\"\n",
        "]\n",
        "\n",
        "test_results = []\n",
        "for test in tests:\n",
        "    tmp_result = BPE_tokenize(test, merges_records)\n",
        "    test_results.append(tmp_result)\n",
        "    print(tmp_result)\n",
        "### DO NOT CHANGE ###\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q235_BPE_tokenize_function', answer = test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neYXGkxgkevr"
      },
      "source": [
        "# End & Submission\n",
        "### Congratulations on finishing your first homework! Here are the deliverables you need to submit to GradeScope\n",
        "- This notebook and py file: rename to `homework1.ipynb` and `homework1.py`. You can download the notebook and py file by going to the top-left corner of this webpage, `File -> Download -> Download .ipynb/.py`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}